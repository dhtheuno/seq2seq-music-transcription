{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from data import run_length_encoding\n",
    "from data import event_codec\n",
    "from data import vocabularies\n",
    "from data import spectrograms\n",
    "from data import note_sequences\n",
    "import note_seq\n",
    "import numpy as np\n",
    "import pretty_midi\n",
    "import sklearn\n",
    "from typing import Callable, Mapping, Optional, Sequence, Tuple\n",
    "import editdistance\n",
    "import mir_eval\n",
    "import librosa\n",
    "from utils import _audio_to_frames, AttrDict\n",
    "from model.ListenAttendSpell import ListenAttendSpell \n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "import functools\n",
    "from evaluation import *\n",
    "import os \n",
    "\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset import MAESTRO, collate_fn, worker_init_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('MAESTRO/maestro/exp/las_model3/config.yaml', 'r') as f:\n",
    "    file_config = yaml.safe_load(f)\n",
    "config = AttrDict(file_config)\n",
    "config.training.beam_size = 2\n",
    "codec = event_codec.Codec(\n",
    "        max_shift_steps=300,\n",
    "        steps_per_second=100,\n",
    "        event_ranges=[\n",
    "                event_codec.EventRange('pitch', note_seq.MIN_MIDI_PITCH,\n",
    "                            note_seq.MAX_MIDI_PITCH),\n",
    "                event_codec.EventRange('velocity', 0, 127)\n",
    "        ])\n",
    "vocab = vocabularies.vocabulary_from_codec(codec)\n",
    "spectrogram_config = spectrograms.SpectrogramConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_states = torch.load(\"MAESTRO/maestro/exp/las_model3/las_model3.epoch6999.chkpt\", map_location=torch.device('cuda'))\n",
    "model = ListenAttendSpell.load_model(model_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_files, midi_files = get_test_files('/home/ubuntu/seq2seq-music-transcription/data/MAESTRO/maestro-v1.0.0.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = final_files[0].replace(\"_test.pt\", \".wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented_frames, segmented_times = segmentize_audio(audio_path, spectrogram_config)\n",
    "segmented_spec=[]\n",
    "for i in segmented_frames:\n",
    "    j = make_spectrogram(i, spectrogram_config)\n",
    "    segmented_spec.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 135/251 [04:20<03:43,  1.93s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_22403/459075624.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpred_ns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_pred_ns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msegmented_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegmented_times\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcodec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/seq2seq-music-transcription/evaluation.py\u001b[0m in \u001b[0;36mmake_pred_ns\u001b[0;34m(audio_data, times, codec, vocab, model, config, is_gpu)\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0;31m#input_length = input_length.cuda()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecognize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m         \u001b[0;31m#print(output)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mevents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/seq2seq-music-transcription/model/ListenAttendSpell.py\u001b[0m in \u001b[0;36mrecognize\u001b[0;34m(self, input, input_length, config, batch, gpu)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrecognize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgreedy_decoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/seq2seq-music-transcription/model/encoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, input_lengths)\u001b[0m\n\u001b[1;32m     31\u001b[0m             batch_first = True)\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mpacked_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         outputs, _ = pad_packed_sequence(\n\u001b[1;32m     35\u001b[0m             \u001b[0mpacked_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    692\u001b[0m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[1;32m    693\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 694\u001b[0;31m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n\u001b[0m\u001b[1;32m    695\u001b[0m                               self.num_layers, self.dropout, self.training, self.bidirectional)\n\u001b[1;32m    696\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pred_ns = make_pred_ns(segmented_spec, segmented_times, codec, vocab, model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "midi_path = '/Users/donghyunlee/Desktop/encoder_decoder_model/test_data/MIDI-Unprocessed_SMF_02_R1_2004_01-05_ORIG_MID--AUDIO_02_R1_2004_05_Track05_wav.midi'\n",
    "ref_ns = note_seq.midi_file_to_note_sequence(midi_path)\n",
    "note_sequences.validate_note_sequence(ref_ns)\n",
    "ref_ns = note_seq.apply_sustain_control_changes(ref_ns)\n",
    "output_1 = evaluation_mir_eval(ref_ns, pred_ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Onset precision': 0.9843219231774236,\n",
       " 'Onset recall': 0.9543957436027363,\n",
       " 'Onset F1': 0.9691278621044507,\n",
       " 'Onset + offset precision': 0.7123072903057225,\n",
       " 'Onset + offset recall': 0.6906511274385609,\n",
       " 'Onset + offset F1': 0.7013120658605608,\n",
       " 'Onset + velocity precision': 0.9619806637052521,\n",
       " 'Onset + velocity recall': 0.932733721814036,\n",
       " 'Onset + velocity F1': 0.9471314638538719,\n",
       " 'Onset + offset + velocity precision': 0.6925790436373138,\n",
       " 'Onset + offset + velocity recall': 0.6715226754497087,\n",
       " 'Onset + offset + velocity F1': 0.6818883457679443}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BATCH TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_files, midi_files = get_test_files('/home/ubuntu/seq2seq-music-transcription/data/MAESTRO/maestro-v1.0.0.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ListenAttendSpell(\n",
       "  (encoder): Encoder(\n",
       "    (rnn): LSTM(512, 512, num_layers=4, batch_first=True, dropout=0.2, bidirectional=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(560, 640)\n",
       "    (rnn): ModuleList(\n",
       "      (0): LSTMCell(1664, 1024)\n",
       "    )\n",
       "    (attention): DotProductAttention()\n",
       "    (linear): Sequential(\n",
       "      (0): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=1024, out_features=560, bias=True)\n",
       "    )\n",
       "    (crit): CrossEntropyLoss()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cuda()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = final_files[4]\n",
    "data = torch.load(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_batch_ns = make_batch_pred_ns(data, config, codec, vocab, model, is_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/mir_eval/transcription_velocity.py:185: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  slope, intercept = np.linalg.lstsq(\n"
     ]
    }
   ],
   "source": [
    "midi_path = midi_files[4]\n",
    "ref_ns = note_seq.midi_file_to_note_sequence(midi_path)\n",
    "note_sequences.validate_note_sequence(ref_ns)\n",
    "ref_ns = note_seq.apply_sustain_control_changes(ref_ns)\n",
    "output_1 = evaluation_mir_eval(ref_ns, greedy_batch_ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Onset precision': 0.9388166741371582,\n",
       " 'Onset recall': 0.8748955722639933,\n",
       " 'Onset F1': 0.9057297297297296,\n",
       " 'Onset + offset precision': 0.7138054683998207,\n",
       " 'Onset + offset recall': 0.6652046783625731,\n",
       " 'Onset + offset F1': 0.6886486486486487,\n",
       " 'Onset + velocity precision': 0.9110264455401166,\n",
       " 'Onset + velocity recall': 0.8489974937343359,\n",
       " 'Onset + velocity F1': 0.8789189189189189,\n",
       " 'Onset + offset + velocity precision': 0.6943074854325415,\n",
       " 'Onset + offset + velocity recall': 0.6470342522974102,\n",
       " 'Onset + offset + velocity F1': 0.6698378378378379}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 101/125 [15:51<05:00, 12.51s/it]"
     ]
    }
   ],
   "source": [
    "final_output = []\n",
    "for audio, midi in tqdm(zip(final_files, midi_files), total=len(final_files)):\n",
    "    data = torch.load(audio)\n",
    "    greedy_batch_ns = make_batch_pred_ns(data, config, codec, vocab, model, is_gpu=True)\n",
    "    midi_path = midi\n",
    "    ref_ns = note_seq.midi_file_to_note_sequence(midi_path)\n",
    "    note_sequences.validate_note_sequence(ref_ns)\n",
    "    ref_ns = note_seq.apply_sustain_control_changes(ref_ns)\n",
    "    output = evaluation_mir_eval(ref_ns, greedy_batch_ns)\n",
    "    final_output.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "onset_f1 = []\n",
    "onset_offset_f1 = []\n",
    "onset_offset_velocity_f1 = []\n",
    "\n",
    "for output in final_output:\n",
    "    onset_f1.append(output['Onset F1'])\n",
    "    onset_offset_f1.append(output['Onset + offset F1'])\n",
    "    onset_offset_velocity_f1.append(output['Onset + offset + velocity F1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "plot_data = [onset_f1, onset_offset_f1, onset_offset_velocity_f1]\n",
    "df = pd.DataFrame(plot_data).transpose()\n",
    "df.columns= ['Onset F1', 'Onset + offset F1','Onset + offset + velocity F1']\n",
    "boxplot = df.boxplot(column=['Onset F1', 'Onset + offset F1','Onset + offset + velocity F1'],figsize=(8,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.8363319280466646, [0.960677966101695, (array([88]),)], [0.6900780379041248, (array([21]),)]], [0.5350215212155319, [0.7827658933413679, (array([13]),)], [0.33221850613154963, (array([21]),)]], [0.5038448972309341, [0.7477966101694915, (array([88]),)], [0.29919950098762865, (array([67]),)]]]\n"
     ]
    }
   ],
   "source": [
    "info_data = []\n",
    "for i in plot_data:\n",
    "    mean = np.mean(i)\n",
    "    max = np.max(i)\n",
    "    max_index = np.where(i==max)\n",
    "    min = np.min(i)\n",
    "    min_index = np.where(i==min)\n",
    "    info_data.append([mean,[max, max_index],[min, min_index]])\n",
    "\n",
    "print(info_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/seq2seq-music-transcription/data/MAESTRO/2017/MIDI-Unprocessed_053_PIANO053_MID--AUDIO-split_07-06-17_Piano-e_3-04_wav--1.midi\n",
      "/home/ubuntu/seq2seq-music-transcription/data/MAESTRO/2017/MIDI-Unprocessed_066_PIANO066_MID--AUDIO-split_07-07-17_Piano-e_3-02_wav--2.midi\n",
      "/home/ubuntu/seq2seq-music-transcription/data/MAESTRO/2013/ORIG-MIDI_01_7_7_13_Group__MID--AUDIO_11_R1_2013_wav--4.midi\n",
      "/home/ubuntu/seq2seq-music-transcription/data/MAESTRO/2008/MIDI-Unprocessed_15_R2_2008_01-04_ORIG_MID--AUDIO_15_R2_2008_wav--4.midi\n",
      "0.7829610162067455\n",
      "0.5596912048524952\n",
      "0.5234812976748461\n"
     ]
    }
   ],
   "source": [
    "print(midi_files[88])\n",
    "print(midi_files[100])\n",
    "print(midi_files[33])\n",
    "print(midi_files[21])\n",
    "print(onset_f1[21])\n",
    "print(onset_offset_f1[33])\n",
    "print(onset_offset_velocity_f1[33])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10344/3305994542.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnote_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnote_sequence_to_pretty_midi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minst\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstruments\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0minst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_drum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ns' is not defined"
     ]
    }
   ],
   "source": [
    "pm = note_seq.note_sequence_to_pretty_midi(ns)\n",
    "for inst in pm.instruments:\n",
    "    inst.is_drum = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10344/703529645.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mref_pianoroll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_piano_roll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m62.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_piano_roll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_pitch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_pitch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m62.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# Use librosa's specshow function for displaying the piano roll\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     librosa.display.specshow(pm.get_piano_roll(fs)[start_pitch:end_pitch],\n\u001b[1;32m      5\u001b[0m                              \u001b[0mhop_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cqt_note'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pm' is not defined"
     ]
    }
   ],
   "source": [
    "ref_pianoroll = pm.get_piano_roll(fs=62.5)\n",
    "def plot_piano_roll(pm, start_pitch, end_pitch, fs=62.5):\n",
    "    # Use librosa's specshow function for displaying the piano roll\n",
    "    librosa.display.specshow(pm.get_piano_roll(fs)[start_pitch:end_pitch],\n",
    "                             hop_length=1, sr=fs, x_axis='time', y_axis='cqt_note',\n",
    "                             fmin=pretty_midi.note_number_to_hz(start_pitch))\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plot_piano_roll(pm, 0, 127)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_midi=[33,21,100, 88]\n",
    "for i in check_midi:\n",
    "    data = torch.load(final_files[i])\n",
    "    greedy_batch_ns = make_batch_pred_ns(data, config, codec, vocab, model, is_gpu=True)\n",
    "    note_seq.note_sequence_to_midi_file(greedy_batch_ns, f\"{i}_predicted.mid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for audio, midi in tqdm(zip(final_files, midi_files), total=len(final_files)):\n",
    "    data = torch.load(audio)\n",
    "    greedy_batch_ns = make_batch_pred_ns(data, config, codec, vocab, model, is_gpu=True)\n",
    "    midi_path = midi\n",
    "    ref_ns = note_seq.midi_file_to_note_sequence(midi_path)\n",
    "    note_sequences.validate_note_sequence(ref_ns)\n",
    "    ref_ns = note_seq.apply_sustain_control_changes(ref_ns)\n",
    "    output = evaluation_mir_eval(ref_ns, greedy_batch_ns)\n",
    "    final_output.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "codec = event_codec.Codec(\n",
    "                max_shift_steps=config.data.max_shift_steps,\n",
    "                steps_per_second=config.data.steps_per_second,\n",
    "                event_ranges=[\n",
    "                    event_codec.EventRange('pitch', note_seq.MIN_MIDI_PITCH,\n",
    "                                note_seq.MAX_MIDI_PITCH),\n",
    "                    event_codec.EventRange('velocity', 0, 127)\n",
    "                ])\n",
    "run_length_encode_shifts = run_length_encoding.run_length_encode_shifts_fn(\n",
    "        codec=codec)\n",
    "vocab = vocabularies.vocabulary_from_codec(codec)\n",
    "spectrogram_config = spectrograms.SpectrogramConfig()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5b2c14c5f2a3b21e6c2412c8196f5145870350e81c0b737cae3e5c60eb1e1eac"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('pytorch_p38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
