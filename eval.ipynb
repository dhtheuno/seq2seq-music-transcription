{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from data import run_length_encoding\n",
    "from data import event_codec\n",
    "from data import vocabularies\n",
    "from data import spectrograms\n",
    "from data import note_sequences\n",
    "import note_seq\n",
    "import numpy as np\n",
    "import pretty_midi\n",
    "import sklearn\n",
    "from typing import Callable, Mapping, Optional, Sequence, Tuple\n",
    "import editdistance\n",
    "import mir_eval\n",
    "import librosa\n",
    "from utils import _audio_to_frames, AttrDict\n",
    "from model.ListenAttendSpell import ListenAttendSpell \n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "import functools\n",
    "from evaluation import *\n",
    "import os \n",
    "\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config/model.yaml', 'r') as f:\n",
    "    file_config = yaml.safe_load(f)\n",
    "config = AttrDict(file_config)\n",
    "config.training.beam_size = 2\n",
    "codec = event_codec.Codec(\n",
    "        max_shift_steps=300,\n",
    "        steps_per_second=100,\n",
    "        event_ranges=[\n",
    "                event_codec.EventRange('pitch', note_seq.MIN_MIDI_PITCH,\n",
    "                            note_seq.MAX_MIDI_PITCH),\n",
    "                event_codec.EventRange('velocity', 0, 127)\n",
    "        ])\n",
    "vocab = vocabularies.vocabulary_from_codec(codec)\n",
    "spectrogram_config = spectrograms.SpectrogramConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_states = torch.load(\"/home/ubuntu/seq2seq-music-transcription/MAESTRO/maestro/exp/las_model/las_model.epoch6799.chkpt\", map_location=torch.device('cuda'))\n",
    "model = ListenAttendSpell.load_model(model_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_files, midi_files = get_test_files('/home/ubuntu/seq2seq-music-transcription/data/MAESTRO/maestro-v1.0.0.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = final_files[0].replace(\"_test.pt\", \".wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented_frames, segmented_times = segmentize_audio(audio_path, spectrogram_config)\n",
    "segmented_spec=[]\n",
    "for i in segmented_frames:\n",
    "    j = make_spectrogram(i, spectrogram_config)\n",
    "    segmented_spec.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 135/251 [04:20<03:43,  1.93s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_22403/459075624.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpred_ns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_pred_ns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msegmented_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegmented_times\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcodec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/seq2seq-music-transcription/evaluation.py\u001b[0m in \u001b[0;36mmake_pred_ns\u001b[0;34m(audio_data, times, codec, vocab, model, config, is_gpu)\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0;31m#input_length = input_length.cuda()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecognize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m         \u001b[0;31m#print(output)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mevents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/seq2seq-music-transcription/model/ListenAttendSpell.py\u001b[0m in \u001b[0;36mrecognize\u001b[0;34m(self, input, input_length, config, batch, gpu)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrecognize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgreedy_decoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/seq2seq-music-transcription/model/encoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, input_lengths)\u001b[0m\n\u001b[1;32m     31\u001b[0m             batch_first = True)\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mpacked_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         outputs, _ = pad_packed_sequence(\n\u001b[1;32m     35\u001b[0m             \u001b[0mpacked_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    692\u001b[0m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[1;32m    693\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 694\u001b[0;31m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n\u001b[0m\u001b[1;32m    695\u001b[0m                               self.num_layers, self.dropout, self.training, self.bidirectional)\n\u001b[1;32m    696\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pred_ns = make_pred_ns(segmented_spec, segmented_times, codec, vocab, model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "midi_path = '/Users/donghyunlee/Desktop/encoder_decoder_model/test_data/MIDI-Unprocessed_SMF_02_R1_2004_01-05_ORIG_MID--AUDIO_02_R1_2004_05_Track05_wav.midi'\n",
    "ref_ns = note_seq.midi_file_to_note_sequence(midi_path)\n",
    "note_sequences.validate_note_sequence(ref_ns)\n",
    "ref_ns = note_seq.apply_sustain_control_changes(ref_ns)\n",
    "output_1 = evaluation_mir_eval(ref_ns, pred_ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Onset precision': 0.9843219231774236,\n",
       " 'Onset recall': 0.9543957436027363,\n",
       " 'Onset F1': 0.9691278621044507,\n",
       " 'Onset + offset precision': 0.7123072903057225,\n",
       " 'Onset + offset recall': 0.6906511274385609,\n",
       " 'Onset + offset F1': 0.7013120658605608,\n",
       " 'Onset + velocity precision': 0.9619806637052521,\n",
       " 'Onset + velocity recall': 0.932733721814036,\n",
       " 'Onset + velocity F1': 0.9471314638538719,\n",
       " 'Onset + offset + velocity precision': 0.6925790436373138,\n",
       " 'Onset + offset + velocity recall': 0.6715226754497087,\n",
       " 'Onset + offset + velocity F1': 0.6818883457679443}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BATCH TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_files, midi_files = get_test_files('/home/ubuntu/seq2seq-music-transcription/data/MAESTRO/maestro-v1.0.0.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ListenAttendSpell(\n",
       "  (encoder): Encoder(\n",
       "    (rnn): LSTM(512, 1024, num_layers=4, batch_first=True, dropout=0.2, bidirectional=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(560, 512)\n",
       "    (rnn): ModuleList(\n",
       "      (0): LSTMCell(2560, 2048)\n",
       "    )\n",
       "    (attention): DotProductAttention()\n",
       "    (linear): Sequential(\n",
       "      (0): Linear(in_features=4096, out_features=2048, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=2048, out_features=560, bias=True)\n",
       "    )\n",
       "    (loss): CrossEntropyLoss()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cuda()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = final_files[4]\n",
    "data = torch.load(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_batch_ns = make_batch_pred_ns(data, config, codec, vocab, model, is_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/mir_eval/transcription_velocity.py:185: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  slope, intercept = np.linalg.lstsq(\n"
     ]
    }
   ],
   "source": [
    "midi_path = midi_files[4]\n",
    "ref_ns = note_seq.midi_file_to_note_sequence(midi_path)\n",
    "note_sequences.validate_note_sequence(ref_ns)\n",
    "ref_ns = note_seq.apply_sustain_control_changes(ref_ns)\n",
    "output_1 = evaluation_mir_eval(ref_ns, greedy_batch_ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Onset precision': 0.9500348594004183,\n",
       " 'Onset recall': 0.8538011695906432,\n",
       " 'Onset F1': 0.899351006489935,\n",
       " 'Onset + offset precision': 0.6943992563327911,\n",
       " 'Onset + offset recall': 0.6240601503759399,\n",
       " 'Onset + offset F1': 0.6573534264657354,\n",
       " 'Onset + velocity precision': 0.9140134789681618,\n",
       " 'Onset + velocity recall': 0.8214285714285714,\n",
       " 'Onset + velocity F1': 0.8652513474865251,\n",
       " 'Onset + offset + velocity precision': 0.6693004880316059,\n",
       " 'Onset + offset + velocity recall': 0.6015037593984962,\n",
       " 'Onset + offset + velocity F1': 0.6335936640633594}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/125 [00:00<?, ?it/s]/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/mir_eval/transcription_velocity.py:185: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  slope, intercept = np.linalg.lstsq(\n",
      "100%|██████████| 125/125 [21:38<00:00, 10.39s/it]\n"
     ]
    }
   ],
   "source": [
    "final_output = []\n",
    "for audio, midi in tqdm(zip(final_files, midi_files), total=len(final_files)):\n",
    "    data = torch.load(audio)\n",
    "    greedy_batch_ns = make_batch_pred_ns(data, config, codec, vocab, model, is_gpu=True)\n",
    "    midi_path = midi\n",
    "    ref_ns = note_seq.midi_file_to_note_sequence(midi_path)\n",
    "    note_sequences.validate_note_sequence(ref_ns)\n",
    "    ref_ns = note_seq.apply_sustain_control_changes(ref_ns)\n",
    "    output = evaluation_mir_eval(ref_ns, greedy_batch_ns)\n",
    "    final_output.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "onset_f1 = []\n",
    "onset_offset_f1 = []\n",
    "onset_offset_velocity_f1 = []\n",
    "\n",
    "for output in final_output:\n",
    "    onset_f1.append(output['Onset F1'])\n",
    "    onset_offset_f1.append(output['Onset + offset F1'])\n",
    "    onset_offset_velocity_f1.append(output['Onset + offset + velocity F1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fuck', [100]]\n"
     ]
    }
   ],
   "source": [
    "test = [[0,2,3,4,5,56],[100]]\n",
    "check = []\n",
    "for i in test:\n",
    "    try: \n",
    "        test=i.index(100)\n",
    "        check.append(i)\n",
    "    except:\n",
    "        check.append(\"fuck\")\n",
    "print(check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "plot_data = [onset_f1, onset_offset_f1, onset_offset_velocity_f1]\n",
    "df = pd.DataFrame(plot_data).transpose()\n",
    "df.columns= ['Onset F1', 'Onset + offset F1','Onset + offset + velocity F1']\n",
    "boxplot = df.boxplot(column=['Onset F1', 'Onset + offset F1','Onset + offset + velocity F1'],figsize=(8,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9005913316701326, [0.9769462078182426, (array([88]),)], [0.7780534877309071, (array([33]),)]], [0.6580888297622013, [0.8396271132880392, (array([100]),)], [0.4502847130968024, (array([21]),)]], [0.6359060123798479, [0.8332776478449715, (array([88]),)], [0.4231274638633377, (array([21]),)]]]\n"
     ]
    }
   ],
   "source": [
    "info_data = []\n",
    "for i in plot_data:\n",
    "    mean = np.mean(i)\n",
    "    max = np.max(i)\n",
    "    max_index = np.where(i==max)\n",
    "    min = np.min(i)\n",
    "    min_index = np.where(i==min)\n",
    "    info_data.append([mean,[max, max_index],[min, min_index]])\n",
    "\n",
    "print(info_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/seq2seq-music-transcription/data/MAESTRO/2017/MIDI-Unprocessed_053_PIANO053_MID--AUDIO-split_07-06-17_Piano-e_3-04_wav--1.midi\n",
      "/home/ubuntu/seq2seq-music-transcription/data/MAESTRO/2017/MIDI-Unprocessed_066_PIANO066_MID--AUDIO-split_07-07-17_Piano-e_3-02_wav--2.midi\n",
      "/home/ubuntu/seq2seq-music-transcription/data/MAESTRO/2013/ORIG-MIDI_01_7_7_13_Group__MID--AUDIO_11_R1_2013_wav--4.midi\n",
      "/home/ubuntu/seq2seq-music-transcription/data/MAESTRO/2008/MIDI-Unprocessed_15_R2_2008_01-04_ORIG_MID--AUDIO_15_R2_2008_wav--4.midi\n",
      "0.7829610162067455\n",
      "0.5596912048524952\n",
      "0.5234812976748461\n"
     ]
    }
   ],
   "source": [
    "print(midi_files[88])\n",
    "print(midi_files[100])\n",
    "print(midi_files[33])\n",
    "print(midi_files[21])\n",
    "print(onset_f1[21])\n",
    "print(onset_offset_f1[33])\n",
    "print(onset_offset_velocity_f1[33])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10344/3305994542.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnote_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnote_sequence_to_pretty_midi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minst\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstruments\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0minst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_drum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ns' is not defined"
     ]
    }
   ],
   "source": [
    "pm = note_seq.note_sequence_to_pretty_midi(ns)\n",
    "for inst in pm.instruments:\n",
    "    inst.is_drum = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10344/703529645.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mref_pianoroll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_piano_roll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m62.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_piano_roll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_pitch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_pitch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m62.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# Use librosa's specshow function for displaying the piano roll\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     librosa.display.specshow(pm.get_piano_roll(fs)[start_pitch:end_pitch],\n\u001b[1;32m      5\u001b[0m                              \u001b[0mhop_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cqt_note'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pm' is not defined"
     ]
    }
   ],
   "source": [
    "ref_pianoroll = pm.get_piano_roll(fs=62.5)\n",
    "def plot_piano_roll(pm, start_pitch, end_pitch, fs=62.5):\n",
    "    # Use librosa's specshow function for displaying the piano roll\n",
    "    librosa.display.specshow(pm.get_piano_roll(fs)[start_pitch:end_pitch],\n",
    "                             hop_length=1, sr=fs, x_axis='time', y_axis='cqt_note',\n",
    "                             fmin=pretty_midi.note_number_to_hz(start_pitch))\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plot_piano_roll(pm, 0, 127)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_midi=[33,21,100, 88]\n",
    "for i in check_midi:\n",
    "    data = torch.load(final_files[i])\n",
    "    greedy_batch_ns = make_batch_pred_ns(data, config, codec, vocab, model, is_gpu=True)\n",
    "    note_seq.note_sequence_to_midi_file(greedy_batch_ns, f\"{i}_predicted.mid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for audio, midi in tqdm(zip(final_files, midi_files), total=len(final_files)):\n",
    "    data = torch.load(audio)\n",
    "    greedy_batch_ns = make_batch_pred_ns(data, config, codec, vocab, model, is_gpu=True)\n",
    "    midi_path = midi\n",
    "    ref_ns = note_seq.midi_file_to_note_sequence(midi_path)\n",
    "    note_sequences.validate_note_sequence(ref_ns)\n",
    "    ref_ns = note_seq.apply_sustain_control_changes(ref_ns)\n",
    "    output = evaluation_mir_eval(ref_ns, greedy_batch_ns)\n",
    "    final_output.append(output)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "821f3860854be6a3325f5c12fafde2b8c9741f955fd2d720c03e1dc648762424"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
